In this little example, we're going to see some of the problems that regularly appear in tokenization. Tokenization may seem simple, but it's harder than it first appears. Why is it so hard? Punctuations, contractions (like don't, won't and would've) get in the way. 

We can split text into sentences using punctuation, but unfortunately that's not always going to work. For example, if I wanted to tell you about Dr. Frankenstein, or Mrs. Doubtfire, we'd be in trouble. What if I wanted to write about U.C. Berkeley? When you think about it, URLs like www.google.com are troublesome too. How would we settle on a price of $10.50? The main point is that these punctuation characters serve a variety of purposes in writing. Moreover, the functions they serve change depending on the domain (medical vs forum text) and language.