{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to word embeddings\n",
    "\n",
    "So far we have focused on bag-of-word approaches i.e representations of text as a vector of word frequencies. An alternative formalization of text consists in representing the words (or bi-grams, phrases, etc) themselves as vectors. A _word vector_ has no meaning per se, but it is informative of the _context_ in which the word is used. This vector representation can become very close to the semantic meaning of the word. Combined with simple vector operations, these representations can used to find synonyms, to test analogies, etc. Word vectors can also be used in any subsequent task (dictionary methods, classification, etc) as features instead of the simple word frequencies in the classical bag-of-words approach.\n",
    "\n",
    "In this notebook we are going to construct word embeddings using neural networks. The spirit of the method is to use 'prediction as an excuse': either predict a target word conditional on its surrounding words (_continuous-bag-of-words_) or predict surrounding words conditional on the target (_skip-gram_). What we care about is not the final output but the _hidden layer_ projection from a two-layers neural network designed to solve that prediction problem (see skip-gram diagram below from Mikolov et al. 2013). \n",
    "\n",
    "<img src='img/wordembeddings_diagram.png' />\n",
    "\n",
    "If we choose the hidden layer to have $K$ hidden neurons then each target word is represented by a $K$-dimensional vector of hidden outputm which we call _embedding_. In practice $K$ should be between 100 and 300, but that really depends on the vocabulary size. For the purpose of learning we are going to apply the famous skip-gram Google's Word2Vec approach (Mikolov et al. 2013) to a corpus that is typically _too small_ so that we reduce our word representations to vectors of 30 dimensions. Keep in mind that the typical required vocabulary size is at least half a million of unique tokens.\n",
    "\n",
    "Last note on Word2Vec: it is very powerful! When trained on very large corpora (like all of English Wikipedia) it can perform very strong analogies such as finding that the vector corresponding the most to the output of the operation 'king' - 'man' + 'woman' is 'queen'. There exist alternative packages such as GloVe (Stanford NLP) or FastText."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We are going to embed the vocabulary from the corpus of Frances Ellen Watkins Harper's books we encountered on day 1. First, let's read the files into a single string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "import os\n",
    "DATA_DIR = 'data'\n",
    "\n",
    "import glob\n",
    "fnames = os.path.join(DATA_DIR, 'austen', '*.txt')\n",
    "fnames = glob.glob(fnames)\n",
    "raw = ''\n",
    "for fname in fnames:\n",
    "    with codecs.open(fname, \"r\", encoding='utf-8-sig', errors='ignore') as f:\n",
    "        t = f.read()\n",
    "        raw += t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = raw[1114:] # gets rid of meta information at the beginning\n",
    "\n",
    "# A few modifications before sentence segmentation\n",
    "text = text.replace('Mrs.', 'Mrs')\n",
    "text = text.replace('Mr.', 'Mr')\n",
    "text = text.replace('\\n', ' ')\n",
    "text = text.replace('\\r', ' ')\n",
    "\n",
    "# Sentence segmentation\n",
    "import re\n",
    "sent_boundary_pattern = r'[.?!]'\n",
    "sentences = re.split(sent_boundary_pattern, text)\n",
    "\n",
    "# Remove punctuation, special characters and upper cases\n",
    "from string import punctuation\n",
    "special = ['“', '”']\n",
    "sentences = [''.join([ch for ch in sent if ch not in punctuation and ch not in special]) for sent in sentences]\n",
    "sentences = [sent.lower() for sent in sentences]\n",
    "\n",
    "# Remove white space\n",
    "sentences = [sent.strip() for sent in sentences]\n",
    "\n",
    "# Tokenization within sentence\n",
    "list_of_list = [sent.split() for sent in sentences]\n",
    "list_of_list[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a skip-gram model with Word2Vec \n",
    "\n",
    "First, you'll need to install [Gensim](https://pypi.org/project/gensim/). You can do so directly in the notebook using     ```!pip install```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(min_count=2, vector_size=100, sg=1)\n",
    "model.build_vocab(list_of_list)  # prepare the model vocabulary\n",
    "model.train(list_of_list, total_examples=model.corpus_count, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asses model accuracy\n",
    "### Size of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " print(len(model.wv.index_to_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.get_vector('woman'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.similarity('woman', 'girl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most similar words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.similar_by_word('woman'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = model.wv.get_vector('man') - model.wv.get_vector('boy') + model.wv.get_vector('woman')\n",
    "print(model.wv.similar_by_vector(vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "Try to improve the model by tuning its parameters:\n",
    "- Increase the context window\n",
    "- Change the number of epochs\n",
    "- Change the vector size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro",
   "language": "python",
   "name": "neuro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
